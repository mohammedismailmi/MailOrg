{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef1db5c-b6bf-417f-b676-86dd47227ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1-Score: 0.3086\n",
      "Average Accuracy: 0.1896\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('/Users/mi/Docs/RVU/IML/project/Data In CSV/summerised/summarized_latest_date.csv')\n",
    "\n",
    "# Extract original and summarized text\n",
    "original_texts = df.iloc[:, 4].astype(str)  # Convert to string\n",
    "summarized_texts = df.iloc[:, 7].astype(str)\n",
    "\n",
    "# Function to calculate word-level precision, recall, F1-score, and accuracy\n",
    "def f1_precision_recall_accuracy(original, summary):\n",
    "    original_words = set(original.split())\n",
    "    summary_words = set(summary.split())\n",
    "\n",
    "    if len(summary_words) == 0:\n",
    "        return 0, 0, 0, 0  # Avoid division by zero\n",
    "\n",
    "    precision = len(summary_words & original_words) / len(summary_words)\n",
    "    recall = len(summary_words & original_words) / len(original_words)\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    # New accuracy calculation (word overlap ratio)\n",
    "    accuracy = len(summary_words & original_words) / len(original_words)\n",
    "\n",
    "    return precision, recall, f1, accuracy\n",
    "\n",
    "# Compute F1-scores and accuracy\n",
    "f1_scores, accuracies = [], []\n",
    "\n",
    "for orig, summ in zip(original_texts, summarized_texts):\n",
    "    _, _, f1, accuracy = f1_precision_recall_accuracy(orig, summ)\n",
    "    f1_scores.append(f1)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Display average results\n",
    "print(f\"Average F1-Score: {sum(f1_scores) / len(f1_scores):.4f}\")\n",
    "print(f\"Average Accuracy: {sum(accuracies) / len(accuracies):.4f}\")  # Improved accuracy metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db4ccf2e-5015-4659-ad81-47ae949923d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /Users/mi/Docs/RVU/.venv/lib/python3.11/site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /Users/mi/Docs/RVU/.venv/lib/python3.11/site-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /Users/mi/Docs/RVU/.venv/lib/python3.11/site-packages (from rouge-score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/mi/Docs/RVU/.venv/lib/python3.11/site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: click in /Users/mi/Docs/RVU/.venv/lib/python3.11/site-packages (from nltk->rouge-score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/mi/Docs/RVU/.venv/lib/python3.11/site-packages (from nltk->rouge-score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/mi/Docs/RVU/.venv/lib/python3.11/site-packages (from nltk->rouge-score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/mi/Docs/RVU/.venv/lib/python3.11/site-packages (from nltk->rouge-score) (4.67.1)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25done\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=d3dddaa2f7cfe1d10cb5d4d40ab624ab6c132670b10abadca4c47d2739942bed\n",
      "  Stored in directory: /Users/mi/Library/Caches/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c0c8953-9672-4d1d-92f3-5b8373d61897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1 F1: 0.2829\n",
      "Average ROUGE-2 F1: 0.2368\n",
      "Average ROUGE-L F1: 0.2621\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('/Users/mi/Docs/RVU/IML/project/Data In CSV/summerised/summarized_latest_date.csv')\n",
    "\n",
    "# Extract original and summarized text\n",
    "original_texts = df.iloc[:, 4].astype(str).fillna(\"\")  # Convert to string & handle NaN\n",
    "summarized_texts = df.iloc[:, 7].astype(str).fillna(\"\")\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Compute ROUGE scores\n",
    "rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n",
    "\n",
    "for orig, summ in zip(original_texts, summarized_texts):\n",
    "    scores = scorer.score(orig, summ)\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "# Display average results\n",
    "print(f\"Average ROUGE-1 F1: {sum(rouge1_scores) / len(rouge1_scores):.4f}\")\n",
    "print(f\"Average ROUGE-2 F1: {sum(rouge2_scores) / len(rouge2_scores):.4f}\")\n",
    "print(f\"Average ROUGE-L F1: {sum(rougeL_scores) / len(rougeL_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30f73283-c2f5-42a8-a320-73db4fb88e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1 F1: 0.0030\n",
      "Average ROUGE-2 F1: 0.0000\n",
      "Average ROUGE-L F1: 0.0030\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('/Users/mi/Docs/RVU/IML/project/Data In CSV/new/summerised/months_summerised/summarized_August_2022.csv')\n",
    "\n",
    "# Extract original and summarized text\n",
    "original_texts = df.iloc[:, 4].astype(str).fillna(\"\")  # Convert to string & handle NaN\n",
    "summarized_texts = df.iloc[:, 7].astype(str).fillna(\"\")\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Compute ROUGE scores\n",
    "rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n",
    "\n",
    "for orig, summ in zip(original_texts, summarized_texts):\n",
    "    scores = scorer.score(orig, summ)\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "# Display average results\n",
    "print(f\"Average ROUGE-1 F1: {sum(rouge1_scores) / len(rouge1_scores):.4f}\")\n",
    "print(f\"Average ROUGE-2 F1: {sum(rouge2_scores) / len(rouge2_scores):.4f}\")\n",
    "print(f\"Average ROUGE-L F1: {sum(rougeL_scores) / len(rougeL_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eabb4174-be7c-4621-8c22-0580d9e97417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1 F1: 0.0030\n",
      "Average ROUGE-2 F1: 0.0000\n",
      "Average ROUGE-L F1: 0.0030\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('/Users/mi/Docs/RVU/IML/project/Data In CSV/summerised/months_summerised/new/summarized_August_2022.csv')\n",
    "\n",
    "# Extract original and summarized text\n",
    "original_texts = df.iloc[:, 4].astype(str).fillna(\"\")  # Convert to string & handle NaN\n",
    "summarized_texts = df.iloc[:, 7].astype(str).fillna(\"\")\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Compute ROUGE scores\n",
    "rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n",
    "\n",
    "for orig, summ in zip(original_texts, summarized_texts):\n",
    "    scores = scorer.score(orig, summ)\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "# Display average results\n",
    "print(f\"Average ROUGE-1 F1: {sum(rouge1_scores) / len(rouge1_scores):.4f}\")\n",
    "print(f\"Average ROUGE-2 F1: {sum(rouge2_scores) / len(rouge2_scores):.4f}\")\n",
    "print(f\"Average ROUGE-L F1: {sum(rougeL_scores) / len(rougeL_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be025e11-f277-4f5f-af9d-b469faca9521",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/mi/Docs/RVU/IML/project/Data In CSV/months/clustered_December_2023.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8e310cc-b14d-4973-9c70-55dda5cd17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b24fc6ed-b04a-4340-b080-815ed46653a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()  # Take the [CLS] token embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed80b2f2-3824-4691-a921-0732240961a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"embeddings\"] = df[\"Body\"].apply(lambda x: get_bert_embedding(str(x)))\n",
    "X = np.vstack(df[\"embeddings\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b97da6d-da2a-445f-bd77-ca2da6deb0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 10  # Adjust based on dataset\n",
    "clust_model = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "train_clusters = clust_model.fit_predict(X)\n",
    "test_clusters = clust_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98508004-9c05-450f-92b4-4b5dfd7053d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa199569-6ddf-4483-84d1-515448c1b9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd85b619-27b1-4f7d-9685-7e70fc0e58af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.054259073\n",
      "Davies-Bouldin Index: 2.5749230865113892\n",
      "Calinski-Harabasz Score: 30.59466987170655\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "silhouette = silhouette_score(X, df[\"Cluster\"])\n",
    "davies_bouldin = davies_bouldin_score(X, df[\"Cluster\"])\n",
    "calinski_harabasz = calinski_harabasz_score(X, df[\"Cluster\"])\n",
    "\n",
    "print(\"Silhouette Score:\", silhouette)\n",
    "print(\"Davies-Bouldin Index:\", davies_bouldin)\n",
    "print(\"Calinski-Harabasz Score:\", calinski_harabasz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c01f30f-1dbf-41c5-9a0d-150b7a79dcd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
