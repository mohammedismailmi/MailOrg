{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3c09df-10dc-4350-8b0b-825a94642387",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pandas scikit-learn sentence-transformers hdbscan summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db3c809a-1232-434b-b61c-e798ca8b40f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Subject  \\\n",
      "0                                     Security alert   \n",
      "1                                     Security alert   \n",
      "2         Reminder to attend Wipro Turbo Hiring 2025   \n",
      "3  Your 2025 Application Update from Shiv Nadar U...   \n",
      "4  Intel is hiring Graphics Software Engineering ...   \n",
      "\n",
      "                                             Sender  \\\n",
      "0             Google <no-reply@accounts.google.com>   \n",
      "1             Google <no-reply@accounts.google.com>   \n",
      "2           Team Unstop <noreply@dare2compete.news>   \n",
      "3  Shiv Nadar University <info@study.selfstudys.com   \n",
      "4           Team Unstop <noreply@dare2compete.news>   \n",
      "\n",
      "                                  Receiver       Date Random_id  \\\n",
      "0                     cjsakshi29@gmail.com  2/15/2025     12114   \n",
      "1                     cjsakshi29@gmail.com  2/15/2025     12181   \n",
      "2                     cjsakshi29@gmail.com  2/15/2025     16151   \n",
      "3  C J Sakshi C J S <cjsakshi29@gmail.com>  2/15/2025     11580   \n",
      "4                     cjsakshi29@gmail.com  2/15/2025     21690   \n",
      "\n",
      "                                                Body  \n",
      "0  Google\\r\\nMozilla Thunderbird Email was grante...  \n",
      "1  Google\\r\\nA new sign-in on Mac\\r\\n\\tcjsakshi29...  \n",
      "2  C J Sakshi, Wipro Turbo Hiring 2025.  ﻿ ͏  ﻿ ͏...  \n",
      "3  Shiv Nadar University Chennai 2025 Application...  \n",
      "4  Tap to view details  ﻿ ͏  ﻿ ͏  ﻿ ͏  ﻿ ͏  ﻿ ͏  ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Data In CSV/NewFinal.csv\") \n",
    "print(df.head())  # Check the first few rows of your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93e1e045-65e3-4086-9ecf-7fed33361a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12486, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fefbc27e-e0ea-425d-b65a-2ca80105ad36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vector Shape: (12486, 46721)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Transform the email bodies into TF-IDF vectors\n",
    "tfidf_vectors = vectorizer.fit_transform(df['Body'])\n",
    "\n",
    "# Check the shape of the TF-IDF matrix\n",
    "print(f\"TF-IDF Vector Shape: {tfidf_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7101dc2-c027-4d68-9b2a-66b3d6f1d393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Vector Shape: (12486, 100)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize the email bodies\n",
    "tokenized_emails = [word_tokenize(email) for email in df['Body']]\n",
    "\n",
    "# Train a Word2Vec model (you can also load a pre-trained model if needed)\n",
    "model = Word2Vec(tokenized_emails, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Convert emails into Word2Vec embeddings (averaging word vectors for each email)\n",
    "def get_email_vector(email):\n",
    "    tokens = word_tokenize(email)\n",
    "    word_vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(100)  # Return a zero vector if no word vectors are found\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "email_vectors = np.array([get_email_vector(email) for email in df['Body']])\n",
    "print(f\"Word2Vec Vector Shape: {email_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f7c4f03-30bc-4c18-a9d7-744238fd2cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1739705371.289093  202295 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1739705371.289147  202295 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE Vector Shape: (12486, 512)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "\n",
    "# Load the Universal Sentence Encoder (USE) model\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# Convert email bodies to vectors\n",
    "email_vectors = np.array([embed([email]) for email in df['Body']])\n",
    "\n",
    "# Flatten the vectors\n",
    "email_vectors = email_vectors.squeeze()\n",
    "print(f\"USE Vector Shape: {email_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95865648-d7a4-4a09-8c03-60f77e2de434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectors (first 5 emails):\n",
      "  (0, 19725)\t0.3086262944740503\n",
      "  (0, 28237)\t0.27454937112141536\n",
      "  (0, 40533)\t0.3067876083127798\n",
      "  (0, 16558)\t0.09737329703817545\n",
      "  (0, 19901)\t0.23110544023581028\n",
      "  (0, 5997)\t0.23514243481352606\n",
      "  (0, 6031)\t0.3010225853737425\n",
      "  (0, 12038)\t0.12960773354620883\n",
      "  (0, 19616)\t0.0947021417322963\n",
      "  (0, 12558)\t0.13317980560541207\n",
      "  (0, 15083)\t0.1480889122660237\n",
      "  (0, 19900)\t0.21118275772437778\n",
      "  (0, 11662)\t0.21729610444765\n",
      "  (0, 6151)\t0.3864541967501956\n",
      "  (0, 36298)\t0.13909668615527895\n",
      "  (0, 36305)\t0.13000979786217073\n",
      "  (0, 21461)\t0.08302203359683842\n",
      "  (0, 28521)\t0.16622065814305112\n",
      "  (0, 29491)\t0.08239846360099377\n",
      "  (0, 33863)\t0.11148637930621158\n",
      "  (0, 25465)\t0.11478353217147204\n",
      "  (0, 24639)\t0.1056476312408754\n",
      "  (0, 22132)\t0.12057945647280581\n",
      "  (0, 11555)\t0.13766172880493144\n",
      "  (0, 36535)\t0.10414050887292521\n",
      "  :\t:\n",
      "  (4, 22572)\t0.13341423701620803\n",
      "  (4, 3215)\t0.15735869880406686\n",
      "  (4, 28096)\t0.11126782557409991\n",
      "  (4, 19570)\t0.1567366791543254\n",
      "  (4, 30129)\t0.11065029739346927\n",
      "  (4, 36998)\t0.14984696057946484\n",
      "  (4, 37412)\t0.10264313441740584\n",
      "  (4, 24463)\t0.1714364070855339\n",
      "  (4, 11077)\t0.10778547426540172\n",
      "  (4, 36365)\t0.12305668768110692\n",
      "  (4, 10988)\t0.16912813271378196\n",
      "  (4, 34580)\t0.1704241787550307\n",
      "  (4, 13540)\t0.1628937184551739\n",
      "  (4, 16818)\t0.20680888429561028\n",
      "  (4, 42362)\t0.20241434084285084\n",
      "  (4, 40259)\t0.12937092720097104\n",
      "  (4, 8419)\t0.15592788439935495\n",
      "  (4, 14357)\t0.21501015443847274\n",
      "  (4, 38736)\t0.1478264676088972\n",
      "  (4, 25658)\t0.08452858686261697\n",
      "  (4, 43091)\t0.3717696309574403\n",
      "  (4, 17904)\t0.16944666231770086\n",
      "  (4, 10966)\t0.12337309606749695\n",
      "  (4, 16783)\t0.156942514726992\n",
      "  (4, 12132)\t0.08299066270409718\n"
     ]
    }
   ],
   "source": [
    "print(\"TF-IDF Vectors (first 5 emails):\")\n",
    "print(tfidf_vectors[:5])  # Prints first 5 vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c7015e-28c4-4aae-b18a-8c8f3833b98d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
